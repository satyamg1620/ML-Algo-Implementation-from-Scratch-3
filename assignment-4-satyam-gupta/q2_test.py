# -*- coding: utf-8 -*-
"""Q2_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-h6mR7sy_h7Yi1dKE4xCb8nKprgkVDj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from linearRegression.linear_regression import LinearRegression
from linearRegression.metrics import *
import time
from tabulate import tabulate

import warnings
warnings.warn('Warning Message: This is a warning message')
warnings.simplefilter("ignore")

np.random.seed(45)

# N = 1000
# P = 25
N = 90
P = 10
X = pd.DataFrame(np.random.randn(N, P))
y = pd.Series(np.random.randn(N))

#TODO :  Call the different variants of gradient descent here (as given in Q2)

learning_rates = [0.01, 0.001, 0.0001]
lambdas = [0.5, 0.1, 1]
momentums = [0.1, 0.5, 0.9]

results = []

for learnr in learning_rates:
    for l in lambdas:
        for m in momentums:
            print('Running for lr =', learnr, 'lambda =', l, 'momentum =', m)

            # manual and unregularized
            print('For Manual and Unregularized MSE Loss\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(), lr = learnr)
            penalty = None
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=len(X), gradient_type='manual', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "Manual and Unregularized MSE Loss"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # manual and l2
            print('For Manual and MSE Loss with Ridge Regularization\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(),lr = learnr, alpha = l)
            penalty = 'l2'
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=len(X), gradient_type='manual', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "Manual and MSE Loss with Ridge Regularization"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # JAX and unregularized
            print('For JAX and Unregularized MSE Loss\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(),lr = learnr)
            penalty = None
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=len(X), gradient_type='JAX', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "JAX and Unregularized MSE Loss"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # JAX and l2
            print('For JAX and MSE Loss with Ridge Regularization\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(), lr = learnr, alpha = l)
            penalty = 'l2'
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=len(X), gradient_type='JAX', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "JAX and MSE Loss with Ridge Regularization"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # JAX and l1
            print('For JAX and MSE Loss with LASSO Regularization\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(),lr = learnr, alpha =l)
            penalty = 'l1'
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=len(X), gradient_type='JAX', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "JAX and MSE Loss with LASSO Regularization"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # JAX and l2 and SGD
            print('For JAX and MSE Loss with Ridge Regularization and SGD\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(), lr = learnr, alpha = l)
            penalty = 'l2'
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=1, gradient_type='JAX', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "JAX and MSE Loss with Ridge Regularization and SGD"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # fit_gradient_descent for running minibatch SGD on mse_loss with ridge regularization
            print('For manual and MSE Loss with Ridge Regularization and minibatch SGD\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(), lr = learnr, alpha = l)
            penalty = 'l2'
            start_time = time.time()
            LR.fit_gradient_descent(batch_size=10, gradient_type='manual', penalty_type=penalty)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "manual and MSE Loss with Ridge Regularization and minibatch SGD"
            moment = None
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            # l2 and SGD and momentum
            print('For JAX and MSE Loss with Ridge Regularization and SGD with momentum\n')
            LR = LinearRegression(fit_intercept=True, X=X.copy(), y=y.copy(), lr = learnr, alpha =l)
            penalty = 'l2'
            start_time = time.time()
            LR.fit_SGD_with_momentum(penalty=penalty, beta = m)
            elapsed_time = time.time() - start_time
            y_hat = LR.predict(X)
            rmse_error = rmse(y_hat, y)
            mae_error = mae(y_hat, y)
            Method = "JAX and MSE Loss with Ridge Regularization and SGD with momentum"
            moment = m
            results.append([learnr, l, moment, elapsed_time, rmse_error, mae_error,Method])
            
            

results_df = pd.DataFrame(results, columns=['LR', 'L', 'Mom', 'Time', 'RMSE', 'MAE', 'METHOD'])

print(results_df.to_markdown())


best_hyperparams = results_df.iloc[results_df['RMSE'].idxmin()]
print('Best hyperparameters:', best_hyperparams)
