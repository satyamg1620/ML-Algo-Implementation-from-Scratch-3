# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax
from sklearn.linear_model import LinearRegression as LR
from linearRegression.metrics import rmse, mae, predict, rss, mse

import jax.numpy as jnp
from jax import grad, jit, vmap
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor
import mpl_toolkits.mplot3d.axes3d as p3

import warnings
warnings.warn('Warning Message: This is a warning message')
warnings.simplefilter("ignore")

np.random.seed(45)

class LinearRegression():
  def __init__(self, fit_intercept:bool=False, X:pd.DataFrame=None, y:pd.DataFrame=None, alpha:float=0.0, lr:float=0.001, num_iters=100):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.coef_ = np.array([]) #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=pd.DataFrame([]) # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    
    self.X:pd.DataFrame = X 
    self.y:pd.Series = y 
    self.alpha:float = alpha
    self.lr = lr
    self.batch_X = None
    self.batch_y = None
    self.num_iters = num_iters
    pass
  

  def fit_sklearn_LR(self, X:pd.DataFrame, y:pd.Series):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    reg = LR(fit_intercept=self.fit_intercept, n_jobs=1).fit(X, y)
    self.coef_ = reg.coef_ 
    if self.fit_intercept:
      self.coef_ = np.insert(self.coef_, 0, reg.intercept_)
    pass
  
  def fit_normal_equations(self, X, y):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2

    # Add a column of ones for the intercept term
    if self.fit_intercept:
      X = np.hstack((np.ones((X.shape[0], 1)), X))

    # Compute the normal equation
    x_T_x = np.dot(X.T, X)
    x_T_y = np.dot(X.T, y)
    w = np.linalg.solve(x_T_x, x_T_y)

    # Assigning the regression coefficients
    self.coef_ = w
    pass
  

  def fit_SVD(self, X, y):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix

    if self.fit_intercept:
      X = np.hstack((np.ones((X.shape[0], 1)), X))

    # Compute the SVD of the coefficient matrix
    u, s, vh = np.linalg.svd(X)

    # Compute the pseudoinverse of the coefficient matrix
    s_inv = np.zeros_like(X.T)
    s_inv[:X.shape[1], :X.shape[1]] = np.diag(1/s)
    x_pinv = np.dot(vh.T, np.dot(s_inv, u.T))

    # Compute the regression coefficients
    w = np.dot(x_pinv, y)

    # Assigning the regression coefficients
    self.coef_ = w
    pass

  def mse_loss(self, y_hat: pd.Series, y: pd.Series):                
    # Compute the MSE loss with the learned model
    assert y_hat.size == y.size
    return (((y_hat - y) ** 2).mean())
    
############################################################
  def compute_gradient(self, penalty):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized

    def predict(X):
      y_pred = np.dot(X, self.coef_)
      return y_pred

    # Unregularized MSE loss gradient
    y_pred = predict(self.batch_X)
    error = y_pred - self.batch_y
    grad_mse = 2.0 * np.dot(self.batch_X.T, error) / self.batch_X.shape[0]

    # Ridge regression (L2 penalty) gradient
    if penalty == 'l2':
        grad_ridge = grad_mse + 2 * self.alpha * self.coef_
        return grad_ridge
    else:
        return grad_mse

    pass

  def compute_jax_gradient(self, penalty):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized

    # Define MSE loss function
    def mse_loss(y_true, y_pred):
        return jnp.mean(jnp.square(y_true - y_pred))
    
    # Define regularized MSE loss function with penalty 'penalty'
    def regularized_loss(w, X, y, penalty, alpha):
        y_pred = jnp.dot(X, w)
        mse = mse_loss(y, y_pred)
        if penalty == 'l1':
            l1_penalty = alpha * jnp.sum(jnp.abs(w))
            return mse + l1_penalty
        elif penalty == 'l2':
            l2_penalty = alpha * jnp.sum(jnp.square(w))
            return mse + l2_penalty
        else:
            return mse
        
    # Define gradient of regularized MSE loss function with penalty 'penalty'
    def gradient(w, X, y, penalty, alpha):
        loss = lambda w: regularized_loss(w, X, y, penalty, alpha)
        return grad(loss)(w)
    
    # Compute gradients for all samples
    gradients = vmap(lambda x, y: gradient(self.coef_, x, y, penalty, self.alpha))(jnp.array(self.batch_X), jnp.array(self.batch_y))
    
    # Average gradients across samples
    mean_gradients = jnp.mean(gradients, axis=0)
    
    return mean_gradients


  def fit_gradient_descent(self, batch_size:int, gradient_type, penalty_type, num_iters:int=20, lr:float=0.01):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
    
    if self.fit_intercept:
      self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))
    if len(self.coef_) == 0:
      self.coef_ = np.zeros(self.X.shape[1])

    # Define batch indices
    batch_indices = np.array_split(np.arange(len(self.X)), len(self.X) // batch_size)
    
    # Perform gradient descent
    for i in range(num_iters):
        count = 0
        for indices in batch_indices:
            count += 1
            # Compute gradient for current batch
            # Define batches
            self.batch_X = self.X[indices].copy()
            self.batch_y = self.y[indices].copy()

            # Define gradient function based on gradient_type
            if gradient_type == 'manual':
                gradient = self.compute_gradient(penalty_type)
            elif gradient_type == 'JAX':
                gradient = self.compute_jax_gradient(penalty_type)
            else:
                raise ValueError("Invalid gradient_type. Must be 'manual' or 'JAX'.")

            self.coef_ -= lr * gradient
            self.all_coef[f'{i}_{count}'] = self.coef_.copy()


  def fit_SGD_with_momentum(self, penalty='l2', beta=0.9):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)

    if self.fit_intercept:
      self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))

  # Initialize the model weights to zeros
    n_features = self.X.shape[1]
    if len(self.all_coef) == 0:
      self.coef_ = np.zeros(self.X.shape[1])

    # Initialize the momentum term to zeros
    v = np.zeros(n_features)

    # Define the regularization term and its gradient
    if penalty == 'l2':
        reg_term = lambda w: self.alpha * np.sum(w ** 2)
        reg_grad = lambda w: 2 * self.alpha * w
    else:
        raise NotImplementedError('Only L2 regularization is currently supported')

    # Perform SGD with momentum
    for i in range(50):
        # Shuffle the training data
        idx = np.random.permutation(self.X.shape[0])
        X_shuffled, y_shuffled = np.array(self.X)[idx], np.array(self.y)[idx]

        # Iterate over the training data and update the model weights
        for j in range(self.X.shape[0]):
            x_j, y_j = X_shuffled[j], y_shuffled[j]

            # Compute the gradient of the loss function
            grad_loss = x_j * (np.dot(x_j, self.coef_) - y_j)

            # Compute the gradient of the regularization term
            grad_reg = reg_grad(self.coef_)

            # Compute the gradient of the total objective
            grad_total = grad_loss + grad_reg

            # Update the momentum term
            v = beta * v + (1 - beta) * grad_total

            # Update the model weights
            self.coef_ = self.coef_ - self.lr * v


  def predict(self, X):
    # Funtion to run the LinearRegression on a test data point
    # Make predictions using the regression coefficients
    if self.fit_intercept:
      X = np.hstack((np.ones((X.shape[0], 1)), X))
    y_pred = np.dot(X, self.coef_)
    return y_pred
    


  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    x_ = np.linspace(max(self.all_coef.loc[0])-10, max(self.all_coef.loc[0])+10, 50)
    y_ = np.linspace(max(self.all_coef.loc[1])-10, max(self.all_coef.loc[1])+10, 50)
    t0, t1 = np.meshgrid(x_, y_)
    Z = np.zeros_like(t0)

    for i in range(t0.shape[0]):
        for j in range(t0.shape[1]):
            y_pred = predict(pd.DataFrame(x_), np.array([t0[i,j], t1[i,j]]), self.fit_intercept)
            Z[i,j] = mse(pd.Series(y_pred), pd.Series(y_))

    fig = plt.figure(figsize=(10,8))
    ax = fig.add_subplot(projection="3d")
    ax.plot_surface(t0, t1, Z, cmap='coolwarm', alpha=0.5)
    ax.set_xlabel('Theta_0')
    ax.set_ylabel('Theta_1')
    ax.set_zlabel('RSS')
    ax.view_init(elev=20, azim=-10)
    
    y_pred = predict(X, np.array([theta_0, theta_1]), self.fit_intercept)
    rs = mse(pd.Series(y_pred), pd.Series(y))
    ax.scatter(theta_0, theta_1, rs, color='black', s=100, label='RSS')
    plt.legend()
    plt.show()
    return fig


  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    # Predict y values based on given theta_0 and theta_1
    y_pred = predict(X, np.array([theta_0, theta_1]), self.fit_intercept)
    plt.scatter(X, y, label='data')
    plt.plot(X, y_pred, label='fit')

    # Add title and legend
    plt.title(f"Line fit for Theta0 = {theta_0:.2f}, Theta1 = {theta_1:.2f}")
    plt.legend()
    
    # plt.show()
    return plt.figure()
  

  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of grad

    
ient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """

    # Define a grid of theta_0 and theta_1 values to evaluate the RSS on
    theta_0_grid, theta_1_grid = np.meshgrid(np.linspace(-10, 10, 100), np.linspace(-10, 10, 100))
    
    # Compute the RSS for each combination of theta_0 and theta_1
    RSS_grid = np.zeros(theta_0_grid.shape)
    for i in range(theta_0_grid.shape[0]):
        for j in range(theta_0_grid.shape[1]):
            RSS_grid[i, j] = np.mean((y - theta_0_grid[i, j] - theta_1_grid[i, j])*2)
    
    # Create a figure and axis object
    fig, ax = plt.subplots()
    
    # Plot the contour lines for the RSS
    ax.contour(theta_0_grid, theta_1_grid, RSS_grid, levels=np.logspace(-1, 3, 10))
    
    # Plot the starting point as a red dot
    ax.plot(theta_0, theta_1, 'ro', markersize=10)
    
    # Set the title and axis labels
    ax.set_title('RSS Contour Plot')
    ax.set_xlabel('theta_0')
    ax.set_ylabel('theta_1')
    
    plt.show()
    # Return the figure object
    return fig
